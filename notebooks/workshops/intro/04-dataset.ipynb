{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Base.Iterators\n",
    "using DataStructures\n",
    "using Dates\n",
    "using JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using cfgrib: FileIndex\n",
    "#  `using x: thing` is equivalent to `from x import y` in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Errors\n",
    "\n",
    "In julia you can create a custom exception by making a type as subtype of `Exception`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct DatasetBuildError <: Exception\n",
    "    error_message::String\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "throw(DatasetBuildError(\"Error for reasons\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct DataSet\n",
    "    dimensions::OrderedDict\n",
    "    variables::OrderedDict\n",
    "    attributes::OrderedDict\n",
    "    encoding::Dict\n",
    "end\n",
    "\n",
    "#  TODO: missing arguments:\n",
    "#    - grib_errors\n",
    "#    - index_path\n",
    "#    - filter_by_keys\n",
    "function DataSet(\n",
    "        path::String;\n",
    "        read_keys::Array{String,1}=String[],\n",
    "        kwargs...\n",
    "    )::DataSet\n",
    "\n",
    "    index_keys = sort([ALL_KEYS..., read_keys...])\n",
    "    index = FileIndex(path, index_keys)\n",
    "\n",
    "    return DataSet(\n",
    "        build_dataset_components(index; read_keys=read_keys, kwargs...)...\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notes on implementation**:\n",
    "\n",
    "Missing arguments:\n",
    "- grib_errors\n",
    "- index_path\n",
    "- filter_by_keys\n",
    "\n",
    "Currently aren't logged as they are in the python version, there is no functionality to save or load an index to/from a file, and I haven't propogated the key filtering through the code.\n",
    "\n",
    "Keep this in mind as we go through the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OnDiskArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct OnDiskArray\n",
    "    grib_path::String\n",
    "    size::Tuple\n",
    "    offsets::OrderedDict\n",
    "    message_lengths::Array{Int, 1} #  TODO: Fix seek offset issue\n",
    "    missing_value::Any\n",
    "    geo_ndim::Int\n",
    "    dtype::Type\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically identical to the python version, however it also stores message_lengths so that `seek` can be used correctly - issue was described previously with `first(index::FileIndex)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_key(key, shape) = Tuple((1:l)[k] for (k, l) in zip(key, shape))\n",
    "\n",
    "Base.size(A::OnDiskArray) = A.size\n",
    "\n",
    "Base.axes(A::OnDiskArray) = Tuple(Base.OneTo(i) for i in size(A))\n",
    "Base.axes(A::OnDiskArray, d::Int) = axes(A)[d]\n",
    "\n",
    "Base.convert(::Type{T}, A::OnDiskArray) where T <: Array = A[repeat([Colon()], length(size(A)))...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we extend base as has been described before.\n",
    "\n",
    "There is the addition of setting an `axes` method, this is a generalisation of some array indexing operations which may be used by some bounds checks and other optimisations. More notes on this in [the Julia docs](https://docs.julialang.org/en/v1/devdocs/offset-arrays/#man-custom-indices-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Use propper `to_indices`, add boundscheck\n",
    "function Base.getindex(obj::OnDiskArray, key...)\n",
    "    expanded_keys = expand_key(key, size(obj))\n",
    "    #  Geograpyh dims (e.g. lat, lon) are on the end and need to be loaded fully\n",
    "    #  so only look at the other dimensions\n",
    "    header_items = expanded_keys[1:end-obj.geo_ndim]\n",
    "    array_field_shape = (\n",
    "        (length(l) for l in header_items)..., size(obj)[end-obj.geo_ndim+1:end]...\n",
    "    )\n",
    "    array_field = Array{obj.dtype}(undef, array_field_shape...)\n",
    "\n",
    "    geo_ndim_idx = repeat([Colon()], obj.geo_ndim)\n",
    "\n",
    "    GribFile(obj.grib_path) do file\n",
    "        message_length_cumsum = cumsum(obj.message_lengths)\n",
    "        for (header_indexes, offset) in pairs(obj.offsets)\n",
    "            array_field_indexes = collect(flatten([\n",
    "                findall(it .== ix)\n",
    "                for (it, ix)\n",
    "                in zip(header_items, header_indexes)\n",
    "            ]))\n",
    "\n",
    "            if length(array_field_indexes) != length(header_indexes)\n",
    "                #  If the index (e.g. [10, 4, 1]) is not in the requested header\n",
    "                #  range (e.g [1:10, 1:4, 2]) then findall will return fewer\n",
    "                #  items than required (e.g 2 instead of 3). Skip these cases\n",
    "                continue\n",
    "            end\n",
    "\n",
    "            offset_message_index = findfirst(message_length_cumsum .> offset) - 1\n",
    "            seek(file, offset_message_index)\n",
    "            message = Message(file)\n",
    "            values = message[\"values\"]\n",
    "            array_field[array_field_indexes..., geo_ndim_idx...] = values\n",
    "        end\n",
    "    end\n",
    "\n",
    "    #  TODO: Weird 'correction', not sure if this the right approach. In the case\n",
    "    #  where the key is like [:, :, *2*, 120, 61] you might have an array field of\n",
    "    #  shape (10, 4, 1, 120, 61), which correctly means that only the 2nd layer\n",
    "    #  was loaded. However this means that the index *2* should now be 1\n",
    "    corrected_key = collect(Any, deepcopy(key))\n",
    "    corrected_key[collect(array_field_shape) .== 1] .= 1\n",
    "    #  TODO: Skipped some sections of the python equivalent code as I don't get\n",
    "    #  what they're for. Should check this out later\n",
    "    #  TODO: Add in missing value substitution\n",
    "    return getindex(array_field, corrected_key...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**notes on implementation** (in addition to comments in code):\n",
    "\n",
    "Currently I'm missing this section from the python equivalent:\n",
    "\n",
    "    array = array_field[(Ellipsis,) + item[-self.geo_ndim :]]\n",
    "    array[array == self.missing_value] = np.nan\n",
    "    for i, it in reversed(list(enumerate(item[: -self.geo_ndim]))):\n",
    "        if isinstance(it, int):\n",
    "            array = array[(slice(None, None, None),) * i + (0,)]\n",
    "\n",
    "The missing value bit isn't there as an oversight, as for the rest, I'm not sure what it's for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Use parametric struct instead of any\n",
    "struct Variable\n",
    "    dimensions::Tuple{Vararg{String}}\n",
    "    data::Union{Number, Array, OnDiskArray}\n",
    "    attributes::Dict{String, Any}\n",
    "end\n",
    "\n",
    "function Base.:(==)(a::Variable, b::Variable)\n",
    "    attributes = a.attributes == b.attributes\n",
    "    dimensions = a.dimensions == b.dimensions\n",
    "    data = a.data == b.data\n",
    "\n",
    "    return attributes && dimensions && data\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the comparison operator `==` as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enforce unique attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Implement filter_by_keys\n",
    "function enforce_unique_attributes(\n",
    "        header_values::OrderedDict{String, T} where T <: Array,\n",
    "        attribute_keys::Array\n",
    "    )\n",
    "    attributes = OrderedDict()\n",
    "    for key in attribute_keys\n",
    "        values = header_values[key]\n",
    "\n",
    "        if length(values) > 1\n",
    "            throw(DatasetBuildError(\n",
    "                \"Attributes are not unique for \" *\n",
    "                \"$key: $(values)\"\n",
    "            ))\n",
    "        end\n",
    "\n",
    "        value = values[1]\n",
    "\n",
    "        if !ismissing(value) && !(value in [\"undef\", \"unknown\"])\n",
    "            attributes[\"GRIB_\" * key] = value\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return attributes\n",
    "end\n",
    "\n",
    "\n",
    "#  TODO: Implement filter_by_keys\n",
    "function enforce_unique_attributes(index::FileIndex, attribute_keys::Array)\n",
    "    attributes = enforce_unique_attributes(\n",
    "        index.header_values, attribute_keys\n",
    "    )\n",
    "\n",
    "    return attributes \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Geography Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function build_geography_coordinates(\n",
    "        index, encode_cf, errors\n",
    "    )\n",
    "\n",
    "    first_message = first(index)\n",
    "    geo_coord_vars = OrderedDict()\n",
    "    grid_type = cfgrib.getone(index, \"gridType\")\n",
    "\n",
    "    if \"geography\" in encode_cf && grid_type in GRID_TYPES_DIMENSION_COORDS\n",
    "\n",
    "        column_major = getone(index, \"jPointsAreConsecutive\") != 0\n",
    "        #  TODO: column/row major has always confused me, not sure if this\n",
    "        #  is the correct approach here. Idea is taken from how GRIB.jl\n",
    "        #  handles reading data with `codes_grib_get_data`:\n",
    "        #  https://github.com/weech/GRIB.jl/blob/5710a1f462e888ad38f6e3b282df3fb953478d1b/src/message.jl#L355\n",
    "        if column_major\n",
    "            geo_dims = (\"latitude\", \"longitude\")\n",
    "            geo_shape = (getone(index, \"Ny\"), getone(index, \"Nx\"))\n",
    "        else\n",
    "            geo_dims = (\"longitude\", \"latitude\")\n",
    "            geo_shape = (getone(index, \"Nx\"), getone(index, \"Ny\"))\n",
    "        end\n",
    "        latitudes = first_message[\"distinctLatitudes\"]\n",
    "        geo_coord_vars[\"latitude\"] = Variable(\n",
    "            (\"latitude\",), latitudes, cfgrib.COORD_ATTRS[\"latitude\"]\n",
    "        )\n",
    "\n",
    "        if latitudes[1] > latitudes[end]\n",
    "            geo_coord_vars[\"latitude\"].attributes[\"stored_direction\"] =\n",
    "                \"decreasing\"\n",
    "        end\n",
    "\n",
    "        geo_coord_vars[\"longitude\"] = Variable(\n",
    "            (\"longitude\",),\n",
    "            first_message[\"distinctLongitudes\"],\n",
    "            cfgrib.COORD_ATTRS[\"longitude\"],\n",
    "        )\n",
    "    elseif \"geography\" in encode_cf && grid_type in GRID_TYPES_2D_NON_DIMENSION_COORDS\n",
    "        throw(\"unimplemented\")\n",
    "    else\n",
    "        geo_dims = (\"values\", )\n",
    "        geo_shape = (getone(index, \"numberOfPoints\"), )\n",
    "        try\n",
    "            latitude = first_message[\"latitudes\"]\n",
    "            geo_coord_vars[\"latitude\"] = Variable(\n",
    "                (\"values\", ),\n",
    "                latitude,\n",
    "                COORD_ATTRS[\"latitude\"]\n",
    "            )\n",
    "\n",
    "            longitude = first_message[\"longitudes\"]\n",
    "            geo_coord_vars[\"longitude\"] = Variable(\n",
    "                (\"values\", ),\n",
    "                longitude,\n",
    "                COORD_ATTRS[\"longitude\"]\n",
    "            )\n",
    "        catch e\n",
    "            rethrow(e)\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return geo_dims, geo_shape, geo_coord_vars\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the code comments, this is... a bit confusing to me.\n",
    "\n",
    "Julia is column major like fortran, not row major like python.\n",
    "\n",
    "This means that when the data is loaded from the disk, the axis are flipped, and the order of the dimensions can also change. So in the current implementation, lat and lon are always present in the opposite order to python. \n",
    "\n",
    "You can see this in the dataset tests where the correct dimensions for the julia object are:\n",
    "\n",
    "```\n",
    "    \"number\"        => 10,\n",
    "    \"time\"          => 4,\n",
    "    \"isobaricInhPa\" => 2,\n",
    "    \"longitude\"     => 120,\n",
    "    \"latitude\"      => 61\n",
    "```\n",
    "\n",
    "In python longitude and latitude would be the other way around.\n",
    "\n",
    "Is this okay? Should I change it to the python style? Keep in mind that this means the most efficient way to access data is to iterate over the columns, not the rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode CF First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function encode_cf_first(\n",
    "        data_var_attrs::OrderedDict,\n",
    "        encode_cf::Tuple{Vararg{String}}=(\"parameter\", \"time\"),\n",
    "        time_dims::Tuple{Vararg{String}}=(\"time\", \"step\")\n",
    "    )\n",
    "\n",
    "    #  NOTE: marking value as `const` just means it cannot be reassigned, the\n",
    "    #  value can still be mutated/appended to, so be carfeul `append!`ing to\n",
    "    #  the constants\n",
    "    coords_map = deepcopy(cfgrib.ENSEMBLE_KEYS)\n",
    "    param_id = get(data_var_attrs, \"GRIB_paramId\", missing)\n",
    "    data_var_attrs[\"long_name\"] = \"original GRIB paramId: $(param_id)\"\n",
    "    data_var_attrs[\"units\"] = \"1\"\n",
    "\n",
    "    if \"parameter\" in encode_cf\n",
    "        if haskey(data_var_attrs, \"GRIB_cfName\")\n",
    "            data_var_attrs[\"standard_name\"] = data_var_attrs[\"GRIB_cfName\"]\n",
    "        end\n",
    "\n",
    "        if haskey(data_var_attrs, \"GRIB_name\")\n",
    "            data_var_attrs[\"long_name\"] = data_var_attrs[\"GRIB_name\"]\n",
    "        end\n",
    "\n",
    "        if haskey(data_var_attrs, \"GRIB_units\")\n",
    "            data_var_attrs[\"units\"] = data_var_attrs[\"GRIB_units\"]\n",
    "        end\n",
    "    end\n",
    "\n",
    "    if \"time\" in encode_cf\n",
    "        if issubset(time_dims, cfgrib.ALL_REF_TIME_KEYS)\n",
    "            append!(coords_map, time_dims)\n",
    "        else\n",
    "            throw(\"time_dims $(time_dims) is not a subset of \" *\n",
    "                  \"$(cfgrib.ALL_REF_TIME_KEYS)\"\n",
    "            )\n",
    "        end\n",
    "    else\n",
    "        append!(coords_map, cfgrib.DATA_TIME_KEYS)\n",
    "    end\n",
    "\n",
    "    append!(coords_map, cfgrib.VERTICAL_KEYS)\n",
    "    append!(coords_map, cfgrib.SPECTRA_KEYS)\n",
    "\n",
    "    return coords_map\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Variable Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: Add filter_by_keys\n",
    "function build_variable_components(\n",
    "        index; encode_cf=(),\n",
    "        errors=\"warn\", squeeze=true, read_keys=String[],\n",
    "        time_dims=(\"time\", \"step\")\n",
    "    )\n",
    "    data_var_attrs_keys = cfgrib.DATA_ATTRIBUTES_KEYS\n",
    "    data_var_attrs_keys = [\n",
    "        data_var_attrs_keys;\n",
    "        get(cfgrib.GRID_TYPE_MAP, index[\"gridType\"][1], [])\n",
    "    ]\n",
    "    data_var_attrs_keys = [data_var_attrs_keys; read_keys]\n",
    "\n",
    "    data_var_attrs = enforce_unique_attributes(index, data_var_attrs_keys)\n",
    "\n",
    "    coords_map = encode_cf_first(data_var_attrs, encode_cf, time_dims)\n",
    "\n",
    "    coord_name_key_map = Dict()\n",
    "    coord_vars = OrderedDict()\n",
    "\n",
    "    for coord_key in coords_map\n",
    "        values = index[coord_key]\n",
    "        if length(values) == 1 && ismissing(values[1])\n",
    "            #  TODO: Add logging\n",
    "            #  @warn \"Missing from GRIB Stream $(coord_key)\"\n",
    "            continue\n",
    "        end\n",
    "\n",
    "        coord_name = coord_key\n",
    "\n",
    "        if (\"vertical\" in encode_cf && coord_key == \"level\"\n",
    "                && haskey(data_var_attrs, \"GRIB_typeOfLevel\"))\n",
    "            coord_name = data_var_attrs[\"GRIB_typeOfLevel\"]\n",
    "            coord_name_key_map[coord_name] = coord_key\n",
    "        end\n",
    "\n",
    "        attributes = Dict(\n",
    "            \"long_name\" => \"original GRIB coordinate for key:\" *\n",
    "                           \"$(coord_key)($(coord_name))\",\n",
    "            \"units\"     => \"1\",\n",
    "        )\n",
    "\n",
    "        merge!(attributes, copy(get(cfgrib.COORD_ATTRS, coord_name, Dict())))\n",
    "\n",
    "        data = sort(\n",
    "            values,\n",
    "            rev=get(attributes, \"stored_direction\", \"none\") == \"decreasing\"\n",
    "        )\n",
    "        dimensions = (coord_name, )\n",
    "\n",
    "        if squeeze && length(values) == 1\n",
    "            data = data[1]\n",
    "            #  Should single values be in an array as well?\n",
    "            # typeof(data) == Array ? nothing : data = [data]\n",
    "            dimensions = ()\n",
    "        end\n",
    "\n",
    "        coord_vars[coord_name] = Variable(dimensions, data, attributes)\n",
    "    end\n",
    "\n",
    "    header_dimensions = Tuple(\n",
    "        d for (d, c)\n",
    "        in pairs(coord_vars)\n",
    "        if !squeeze || length(c.data) > 1\n",
    "    )\n",
    "    #  Loses information on which shape belongs to which dimension\n",
    "    #  doesn't seem to matter though\n",
    "    header_shape = Iterators.flatten(\n",
    "        Tuple(size(coord_vars[d].data) for d in header_dimensions)\n",
    "    )\n",
    "\n",
    "    geo_dims, geo_shape, geo_coord_vars = build_geography_coordinates(\n",
    "        index, encode_cf, errors\n",
    "    )\n",
    "\n",
    "    dimensions = (header_dimensions..., geo_dims...)\n",
    "    shape = (header_shape..., geo_shape...)\n",
    "\n",
    "    merge!(coord_vars, geo_coord_vars)\n",
    "\n",
    "    offsets = OrderedDict{NTuple{length(header_dimensions), Int64}, Int}()\n",
    "    for (header_values, offset) in index.offsets\n",
    "        header_indexes = Array{Int}(undef, length(header_dimensions))\n",
    "        for (i, dim) in enumerate(header_dimensions)\n",
    "            coord_name = get(coord_name_key_map, dim, dim)\n",
    "            coord_idx = findfirst(index.index_keys .== coord_name)\n",
    "            header_value = header_values[coord_idx]\n",
    "            header_indexes[i] = findfirst(coord_vars[dim].data .== header_value)\n",
    "        end\n",
    "\n",
    "        offsets[Tuple(header_indexes)] = offset\n",
    "    end\n",
    "\n",
    "    data = OnDiskArray(\n",
    "        index.grib_path,\n",
    "        shape,\n",
    "        offsets,\n",
    "        index.message_lengths,  #  TODO: Fix seek offset issue\n",
    "        missing,\n",
    "        length(geo_dims),\n",
    "        Float32\n",
    "    )\n",
    "\n",
    "    if haskey(coord_vars, \"time\") && haskey(coord_vars, \"step\")\n",
    "        # add the 'valid_time' secondary coordinate\n",
    "        dims, time_data = build_valid_time(\n",
    "            coord_vars[\"time\"].data,\n",
    "            coord_vars[\"step\"].data\n",
    "        )\n",
    "        attrs = cfgrib.COORD_ATTRS[\"valid_time\"]\n",
    "        coord_vars[\"valid_time\"] = Variable(dims, time_data, attrs)\n",
    "    end\n",
    "\n",
    "    data_var_attrs[\"coordinates\"] = join(keys(coord_vars), \" \")\n",
    "    data_var = Variable(dimensions, data, data_var_attrs)\n",
    "    dims = OrderedDict(\n",
    "        (d => s)\n",
    "        for (d, s)\n",
    "        in zip(dimensions, size(data_var.data))\n",
    "    )\n",
    "\n",
    "    return dims, data_var, coord_vars\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per the comment, the header shape is:\n",
    "\n",
    "```\n",
    "    header_shape = Iterators.flatten(\n",
    "        Tuple(size(coord_vars[d].data) for d in header_dimensions)\n",
    "    )\n",
    "```\n",
    "\n",
    "The flatten call is used as otherwise this would return something like `(10, 4, 2, (120, 61))`, indicating that the longitude and latitude data were loaded 'together'. Flattening it removes this aditional information, which seems fine as it isn't used anywhere. Is this okay?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Dataset Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: logging, filter_by_keys\n",
    "function build_dataset_attributes(index; encoding)\n",
    "    attributes = enforce_unique_attributes(index, GLOBAL_ATTRIBUTES_KEYS)\n",
    "    attributes[\"Conventions\"] = \"CF-1.7\"\n",
    "\n",
    "    if \"GRIB_centreDescription\" in keys(attributes)\n",
    "        attributes[\"institution\"] = attributes[\"GRIB_centreDescription\"]\n",
    "    end\n",
    "\n",
    "    attributes_namespace = Dict(\n",
    "        \"cfgrib_version\" => cfgrib_jl_version,  # TODO: Package versions are experimental, this should be changed: https://julialang.github.io/Pkg.jl/dev/api/#Pkg.dependencies\n",
    "        \"cfgrib_open_kwargs\" => JSON.json(encoding),\n",
    "        \"eccodes_version\" => \"missing\",  # TODO: Not sure how to get this\n",
    "        \"timestamp\" => string(Dates.now()),\n",
    "    )\n",
    "\n",
    "    history_in = (\n",
    "        \"timestamp GRIB to CDM+CF via \" *\n",
    "        \"cfgrib-cfgrib_version/ecCodes-eccodes_version with cfgrib_open_kwargs\"\n",
    "    )\n",
    "\n",
    "    [history_in=replace(history_in, p) for p in attributes_namespace]\n",
    "    #  TODO: Fix quotes, should probably still be double quotes not single\n",
    "    history_in = replace(history_in, \"\\\"\" => \"'\")\n",
    "    attributes[\"history\"] = history_in\n",
    "\n",
    "    return attributes\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build dataset components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # TODO: Add filter_by_keys\n",
    "function build_dataset_components(\n",
    "        index; errors=\"warn\",\n",
    "        encode_cf=(\"parameter\", \"time\", \"geography\", \"vertical\"),\n",
    "        squeeze=true, read_keys=String[], time_dims=(\"time\", \"step\")\n",
    "    )\n",
    "\n",
    "    dimensions = OrderedDict()\n",
    "    variables = OrderedDict()\n",
    "    for param_id in index[\"paramId\"]\n",
    "        var_index = filter(index, paramId=param_id)\n",
    "        dims, data_var, coord_vars = build_variable_components(\n",
    "            var_index;\n",
    "            encode_cf=encode_cf,\n",
    "            errors=errors,\n",
    "            squeeze=squeeze,\n",
    "            read_keys=read_keys,\n",
    "            time_dims=time_dims,\n",
    "        )\n",
    "\n",
    "        short_name = get(data_var.attributes, \"GRIB_shortName\", \"paramId$(param_id)\")\n",
    "        var_name = get(data_var.attributes, \"GRIB_cfVarName\", \"unknown\")\n",
    "\n",
    "        if (\"parameter\" in encode_cf) && !(var_name in (\"unknown\", \"missing\")) && !ismissing(var_name)\n",
    "            short_name = var_name\n",
    "        end\n",
    "\n",
    "        merge!(variables, coord_vars)\n",
    "        merge!(variables, Dict(short_name => data_var))\n",
    "        merge!(dimensions, dims)\n",
    "    end\n",
    "\n",
    "    encoding = Dict(\n",
    "        \"source\" => index.grib_path,\n",
    "        \"filter_by_keys\" => \"not_implemented\",  # TODO: Add filter_by_keys\n",
    "        \"encode_cf\" => encode_cf\n",
    "    )\n",
    "    attributes = build_dataset_attributes(index, encoding=encoding)\n",
    "\n",
    "    return dimensions, variables, attributes, encoding\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
